{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df = pd.read_csv('../data/original_data/listing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRICE DISTRIBUTION\n",
    "# fe_df = df.copy()\n",
    "# fe_df['price'] = pd.to_numeric(fe_df['price'].str.replace('[^.0-9]', ''))\n",
    "# bins = np.arange(0,1000,50)\n",
    "# plt.hist(np.clip(fe_df['price'], bins[0], bins[-1]), bins=bins)\n",
    "# plt.xlabel('Price ($)')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING FEATURE ENGINEERED DATASET USING ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df = df.copy()\n",
    "fe_df['bathrooms'] = fe_df['bathrooms_text']\n",
    "fe_df.drop(columns = 'bathrooms_text', inplace = True)\n",
    "\n",
    "# change percentage columns to decimal\n",
    "fe_df['host_response_rate'] = fe_df['host_response_rate'].str.rstrip('%').astype('float') / 100.0\n",
    "fe_df['host_acceptance_rate'] = fe_df['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0\n",
    "\n",
    "# change price column to float\n",
    "fe_df['price'] = pd.to_numeric(fe_df['price'].str.replace('[^.0-9]', ''))\n",
    "\n",
    "# map binary columns appropriately\n",
    "fe_df['host_is_superhost'] = fe_df['host_is_superhost'].map({'f':0, 't':1})\n",
    "fe_df['host_has_profile_pic'] = fe_df['host_has_profile_pic'].map({'f':0, 't':1})\n",
    "fe_df['host_identity_verified'] = fe_df['host_identity_verified'].map({'f':0, 't':1})\n",
    "fe_df['has_availability'] = fe_df['has_availability'].map({'f':0, 't':1})\n",
    "fe_df['instant_bookable'] = fe_df['instant_bookable'].map({'f':0, 't':1})\n",
    "\n",
    "# map date columns appropriately \n",
    "fe_df['host_since_year'] = pd.to_datetime(fe_df['host_since']).dt.year\n",
    "fe_df['host_since'] = -(pd.to_datetime(fe_df['host_since']) -  pd.to_datetime(\"now\")).dt.days\n",
    "fe_df['first_review'] = -(pd.to_datetime(fe_df['first_review']) -  pd.to_datetime(\"now\")).dt.days\n",
    "fe_df['last_review'] = -(pd.to_datetime(fe_df['last_review']) -  pd.to_datetime(\"now\")).dt.days\n",
    "\n",
    "# clean bathroom column and convert to float\n",
    "fe_df['bathrooms'] = fe_df['bathrooms'].str.extract(r'(\\d+(?:\\.\\d+)?)').astype('float')\n",
    "\n",
    "# impute room type depending on prop type\n",
    "mask1 = (fe_df['property_type']==np.nan) & fe_df['room_type'].isnull()\n",
    "mask2 = (fe_df['property_type'].isin(['Entire residential home', 'Entire townhouse', 'Entire cottage'])) & fe_df['room_type'].isnull()\n",
    "mask3 = (fe_df['property_type']=='Room in boutique hotel') & fe_df['room_type'].isnull()\n",
    "\n",
    "fe_df.loc[mask1, 'room_type'] = 'Private room'\n",
    "fe_df.loc[mask2, 'room_type'] = 'Entire home/apt'\n",
    "fe_df.loc[mask3, 'room_type'] = 'Hotel room'\n",
    "fe_df['room_type'] = fe_df['room_type'].fillna('Private room') # anything else is priv room\n",
    "\n",
    "# imputing missing cols with common sense substitution from other column\n",
    "fe_df['minimum_minimum_nights'] = fe_df['minimum_minimum_nights'].fillna(fe_df['minimum_nights'])\n",
    "fe_df['maximum_maximum_nights'] = fe_df['maximum_maximum_nights'].fillna(fe_df['maximum_nights'])\n",
    "fe_df['availability_365'] = fe_df['availability_365'].fillna(fe_df['availability_90']*4) # can also fiddle with just 1 instead of 4 (did analysis)\n",
    "\n",
    "# feature engineer for not reviewed before hosts\n",
    "fe_df['host_has_not_accepted_before'] = (fe_df['host_acceptance_rate'].isnull()).astype(int)\n",
    "fe_df['has_not_been_reviewed_before'] = (fe_df['first_review'].isnull()).astype(int)\n",
    "fe_df['host_has_not_replied_before'] = (fe_df['host_response_rate'].isnull()).astype(int)\n",
    "\n",
    "# impute nulls with 0 as context appropriate\n",
    "# Edit: changed to 1\n",
    "fe_df['bathrooms'] = fe_df['bathrooms'].fillna(1)\n",
    "\n",
    "# fill na's for host response time\n",
    "fe_df['host_response_time'] = fe_df['host_response_time'].fillna('no response time')\n",
    "\n",
    "# host only has this listing\n",
    "fe_df['host_single_listing'] = (fe_df['host_listings_count']==1).astype('int')\n",
    "\n",
    "# review scores \n",
    "rev_cols = ['review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', \n",
    "            'review_scores_communication', 'review_scores_location', 'review_scores_value']\n",
    "for col in rev_cols:\n",
    "    fe_df[col] = fe_df[col].fillna(value = fe_df['review_scores_rating'])\n",
    "    \n",
    "# bed and bedroom\n",
    "bed_cols = ['beds', 'bedrooms']\n",
    "for col in bed_cols:\n",
    "    fe_df[col] = fe_df[col].fillna(value = 0) \n",
    "\n",
    "# convert license to binary var : exempt vs rest\n",
    "fe_df['license_exempt_flag'] = fe_df['license'].map({'Exempt':1})\n",
    "fe_df['license_exempt_flag'] = fe_df['license_exempt_flag'].fillna(0)\n",
    "fe_df['PID_license_flag'] = [1 if x == True else 0 for x in fe_df[\"license\"].str.lower().str.contains('pid')]\n",
    "fe_df.drop(['license'], axis = 1, inplace = True)\n",
    "\n",
    "# convert dates to time differences + add covid features\n",
    "fe_df['host_joined_after_covid_flag'] = pd.to_datetime(fe_df['host_since']) > pd.to_datetime('2020-02-15')\n",
    "fe_df['host_joined_after_covid_flag'] = fe_df['host_joined_after_covid_flag'].map({False:0, True:1})\n",
    "fe_df['first_rev_after_covid_flag'] = pd.to_datetime(fe_df['first_review']) > pd.to_datetime('2020-02-15')\n",
    "fe_df['first_rev_after_covid_flag'] = fe_df['first_rev_after_covid_flag'].map({False:0, True:1})\n",
    "\n",
    "# fe_df['host_is_pro_managed'] = fe_df['host_name'].apply(check_host)\n",
    "fe_df['host_neigh_provided'] = fe_df['host_neighbourhood'].isnull().astype('int')\n",
    "\n",
    "# drop columns that will not be useful\n",
    "fe_df.drop(['name','host_name', 'host_location', 'host_neighbourhood'], axis = 1, inplace = True)\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# find missing vals in neighborhood cleansed col\n",
    "missing_nb = fe_df[fe_df['neighbourhood_cleansed'].isna()][['neighbourhood_cleansed', 'neighbourhood', 'latitude', 'longitude']]\n",
    "\n",
    "# create list of coordinates from the missing dataframe above\n",
    "filled_nb = list(missing_nb['latitude'].astype('string') + \",\" + missing_nb['longitude'].astype('string'))\n",
    "# drop neighbourhood col\n",
    "fe_df.drop(['neighbourhood'], axis = 'columns', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_df = pd.read_excel('../data/walkscores.xlsx')\n",
    "walk_df.columns = ['neighbourhood_cleansed', 'walkscore']\n",
    "scores = fe_df.merge(walk_df, 'left')[['neighbourhood_cleansed', 'walkscore']]\n",
    "fe_df['walkscore'] = scores['walkscore']\n",
    "\n",
    "# feature engineering for verification column\n",
    "fe_df['verf_cnt'] = [len(method) for method in fe_df[\"host_verifications\"].str.lower().str.replace(\"\\[|\\]|\\'\", \"\").str.split(\",\").values]\n",
    "fe_df['has_facebook'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('facebook')]\n",
    "fe_df['has_jumio'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('jumio')]\n",
    "fe_df['has_email'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('email')]\n",
    "fe_df['has_review'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('review')]\n",
    "fe_df['has_phone'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('phone')]\n",
    "fe_df['has_google'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('google')]\n",
    "fe_df['has_govt'] = [1 if x == True else 0 for x in fe_df[\"host_verifications\"].str.lower().str.contains('government')]\n",
    "\n",
    "# drop host verifications\n",
    "fe_df.drop(['host_verifications'], axis = 'columns', inplace = True)\n",
    "\n",
    "# feature engineering for property type\n",
    "unique_props = ['barn', 'boat', 'cave', 'camper', 'farm stay', 'entire villa', 'entire residential home', 'aparthotel', 'hotel']\n",
    "budget_props = ['bed and breakfast', 'hostel', 'earth', 'room in guesthouse', 'room in residential home']\n",
    "\n",
    "fe_df['unique_prop_type'] = [1 if x == True else 0 for x in fe_df[\"property_type\"].str.lower().str.contains('|'.join(unique_props))]\n",
    "fe_df['budget_prop_type'] = [1 if x == True else 0 for x in fe_df[\"property_type\"].str.lower().str.contains('|'.join(budget_props))]\n",
    "\n",
    "# drop property type now\n",
    "fe_df.drop('property_type', axis = 'columns', inplace = True)\n",
    "\n",
    "# feature engineering 3 new cols - cnt of special amenities\n",
    "hp_amens = ['parking on premise', 'fireplace', 'bathtub', 'private patio or balcony', 'pool', 'cable tv', 'bbq grill', 'hot tub', \n",
    "           'lake access', 'Pool table', 'Sonos Bluetooth sound system', 'Miele', 'walk-in closet', 'barbecue utensils', \n",
    "           'high chair', 'beach_essentials']\n",
    "all_amens = ['Wifi', 'Long term stays allowed', 'Essentials', 'Smoke alarm','Kitchen', 'Washer', 'Hangers', 'Iron', 'Hair dryer', 'Shampoo',\n",
    "       'Hot water', 'TV', 'Heating', 'Dedicated workspace','Dishes and silverware', 'Dryer', 'Refrigerator', 'Microwave',\n",
    "       'Cooking basics', 'Air conditioning']\n",
    "lp_amens = ['free street parking', 'luggage dropoff allowed', 'lock on bedroom', 'lockbox', 'shower gel', 'elevator', 'kettle', 'single level']\n",
    "\n",
    "fe_df['highp_amen_cnt'] = 0\n",
    "fe_df['pop_amen_cnt'] = 0\n",
    "fe_df['lowp_amen_cnt'] = 0\n",
    "\n",
    "for amen in hp_amens:\n",
    "    fe_df.loc[fe_df['amenities'].str.lower().str.contains(amen.lower()), 'highp_amen_cnt'] += 1\n",
    "\n",
    "for amen in all_amens:\n",
    "    fe_df.loc[fe_df['amenities'].str.lower().str.contains(amen.lower()), 'pop_amen_cnt'] += 1\n",
    "        \n",
    "for amen in lp_amens:\n",
    "    fe_df.loc[fe_df['amenities'].str.lower().str.contains(amen.lower()), 'lowp_amen_cnt'] += 1\n",
    "\n",
    "# feature engineering for amenities\n",
    "fe_df['amenities_cnt'] = [len(method) for method in fe_df[\"amenities\"].str.replace(\"\\[|\\]|\\'\", \"\").str.split(\",\").values]\n",
    "\n",
    "# add features for unique and cheap amenities\n",
    "\n",
    "fe_df['amen_prem_park'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('parking on premise')]\n",
    "fe_df['amen_ind_fireplace'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('fireplace')]\n",
    "fe_df['amen_bathtub'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('bathtub')]\n",
    "fe_df['amen_private_patio'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('private patio or balcony')]\n",
    "fe_df['amen_pool'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('pool')]\n",
    "fe_df['amen_cable'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('cable tv')]\n",
    "fe_df['amen_bbq'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('bbq grill')]\n",
    "fe_df['amen_hottub'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('hot tub')]\n",
    "fe_df['amen_bbg_gear'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('barbecue utensils')]\n",
    "fe_df['amen_child_toys'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('books and toys')]\n",
    "fe_df['amen_high_chair'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('high chair')]\n",
    "fe_df['amen_beach_ess'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('beach essentials')]\n",
    "\n",
    "fe_df['amen_st_park'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('free street parking')]\n",
    "fe_df['amen_lug_dropoff'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('luggage dropoff allowed')]\n",
    "fe_df['amen_bedlock'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('lock on bedroom')]\n",
    "fe_df['amen_lockbox'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('lockbox')]\n",
    "fe_df['amen_elevator'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('elevator')]\n",
    "fe_df['amen_singlev'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('single level')]\n",
    "fe_df['amen_kettle'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('kettle')]\n",
    "fe_df['amen_hostgreet'] = [1 if x == True else 0 for x in fe_df[\"amenities\"].str.lower().str.contains('host greets you')]\n",
    "\n",
    "# Calculate proportion of total amenities that are 'higher priced'\n",
    "fe_df['highp_amen_prop'] = round(fe_df['highp_amen_cnt']/fe_df['amenities_cnt'],2)\n",
    "\n",
    "# DUMMIES FOR TOP 60 MOST POPULAR SBURBS\n",
    "pop_suburbs = list(fe_df['neighbourhood_cleansed'].value_counts()[:60].index)\n",
    "\n",
    "for sub in pop_suburbs:\n",
    "    fe_df['sub_' + sub.replace(\" \", \"_\")] = [1 if x == True else 0 for x in fe_df[\"neighbourhood_cleansed\"]==sub]\n",
    "\n",
    "# creating a function for the next part\n",
    "def clean_desc(series):\n",
    "    \"\"\"\n",
    "    This function removes html text from a column (should be the description column)\n",
    "    Inputs: \n",
    "        A series without null values\n",
    "    Outputs:\n",
    "        A bag of words from the series\n",
    "    \"\"\"\n",
    "    html = ['<br /><br />', '<b>', '</b>', '<br />', 'Ä', 'ö', '√', 'Ñ', ';', ':', '<', '>', '/', '\\\\', '=', '-', '¥', 'Äö√Ñ√¥' ]\n",
    "    for tag in html:\n",
    "        series = series.str.replace(tag, '')\n",
    "        \n",
    "    return series\n",
    "\n",
    "# feature engineering using above intuition\n",
    "#fe_df['desc_beach'] = \n",
    "fe_df['description'] = clean_desc(fe_df['description'])\n",
    "\n",
    "fe_df['description_length'] = fe_df['description'].map(str).apply(len)\n",
    "fe_df['desc_beach'] = fe_df['description'].str.lower().str.contains('|'.join(['beach', 'water view', 'waterfront', 'ocean', 'coast'])).map({True:1, False:0, np.nan:0})\n",
    "fe_df['desc_amen'] = fe_df['description'].str.lower().str.contains('|'.join(['spa', 'sauna', 'steam room', 'gym'])).map({True:1, False:0, np.nan:0})\n",
    "fe_df['desc_view'] = fe_df['description'].str.lower().str.contains('|'.join(['view', 'sunset', 'sunrise', 'panorama', 'panoramic', 'infinity', 'levels', 'overlook'])).map({True:1, False:0, np.nan:0})\n",
    "fe_df['desc_lux'] = fe_df['description'].str.lower().str.contains('|'.join(['estate','acre','lounge', 'deck', 'restaurant', 'balcony', 'renovated', 'modern', 'space', 'renovation', 'alfresco', 'designer'])).map({True:1, False:0, np.nan:0})\n",
    "fe_df['desc_transport'] = fe_df['description'].str.lower().str.contains('|'.join(['minute', 'available', 'shared', 'walking','train', 'station', 'bus', 'cbd'])).map({True:1, False:0, np.nan:0})\n",
    "\n",
    "\n",
    "# note: the below were gotten from the correlation matrix as 0 corr \n",
    "    # https://vishesh-gupta.medium.com/correlation-in-xgboost-8afa649bd066\n",
    "    # https://stats.stackexchange.com/questions/266267/should-one-be-concerned-about-multi-collinearity-when-using-non-linear-models\n",
    "\n",
    "cols_to_drop = ['description', 'neighborhood_overview', 'neighbourhood_cleansed', 'host_about', 'amenities', 'host_picture_url',\n",
    "                'id', 'listing_url', 'scrape_id', 'last_scraped', 'picture_url', 'host_id', 'host_url', 'host_thumbnail_url', 'calendar_last_scraped',\n",
    "                'calendar_updated', 'neighbourhood_group_cleansed', \n",
    "                ]\n",
    "\n",
    "# note: the below were gotten from the correlation matrix as 0 corr \n",
    "    # https://vishesh-gupta.medium.com/correlation-in-xgboost-8afa649bd066\n",
    "    # https://stats.stackexchange.com/questions/266267/should-one-be-concerned-about-multi-collinearity-when-using-non-linear-models\n",
    "dist_df = pd.read_excel('final_distances.xlsx', index_col = 0)\n",
    "hway_dist_df = pd.read_excel('hway_distances.xlsx', index_col = 0)\n",
    "\n",
    "fe_df2 = fe_df.drop(cols_to_drop, axis = 1).copy()\n",
    "fe_df_dums = pd.get_dummies(fe_df2)\n",
    "fe_df_dums = fe_df_dums.join(dist_df['dist']).copy()\n",
    "hway_dist_df.columns = ['hway_dist']\n",
    "fe_df_dums = fe_df_dums.join(hway_dist_df['hway_dist']).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis took 10 min  \n",
    "Final distances took 9 mins  \n",
    "Highway distances took 2 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE CODE BELOW TAKES A WHILE TO RUN HENCE I HAVE USED EXCEL INSTEAD\n",
    "# # Get sentiment score for description and neighboorhood overview\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# desc_sentiment = [] # description sentiment\n",
    "# for i in fe_df['description']:\n",
    "#     desc_sentiment.append(SentimentIntensityAnalyzer().polarity_scores(str(i))['compound'])\n",
    "# fe_df['desc_sentiment'] = pd.Series(desc_sentiment)\n",
    "# neigh_sentiment = [] # neighbourhood sentiment\n",
    "# for i in fe_df['neighborhood_overview']:\n",
    "#     neigh_sentiment.append(SentimentIntensityAnalyzer().polarity_scores(str(i))['compound'])\n",
    "# fe_df['neigh_sentiment'] = pd.Series(neigh_sentiment)\n",
    "# host_neigh_sent = [] # host neighbourhood sentiment\n",
    "# for i in fe_df['host_about']:\n",
    "#     host_neigh_sent.append(SentimentIntensityAnalyzer().polarity_scores(str(i))['compound'])\n",
    "# fe_df['host_about_sentiment'] = pd.Series(host_neigh_sent)\n",
    "\n",
    "# fe_df[['desc_sentiment', 'neigh_sentiment', 'host_about_sentiment']].to_excel('sentiment_scores.xlsx')\n",
    "\n",
    "# exp_lat = [-33.977696,\n",
    "# -33.983353,\n",
    "# -33.989201,\n",
    "# -33.991842,\n",
    "# -33.978549,\n",
    "# -33.964214,\n",
    "# -33.939094,\n",
    "# -33.935071,\n",
    "# -33.930274,\n",
    "# -33.920968,\n",
    "# -33.915001,\n",
    "# -33.915849,\n",
    "# -33.913614,\n",
    "# -33.911022,\n",
    "# -33.906451,\n",
    "# -33.903587,\n",
    "# -33.891459,\n",
    "# -33.892676,\n",
    "# -33.879146,\n",
    "# -33.873718,\n",
    "# -33.865029,\n",
    "# -33.859503,\n",
    "# -33.850456,\n",
    "# -33.844058,\n",
    "# -33.835353,\n",
    "# -33.839878,\n",
    "# -33.843046,\n",
    "# -33.847018,\n",
    "# -33.852903,\n",
    "# -33.850651,\n",
    "# -33.868973,\n",
    "# -33.869690,\n",
    "# -33.865112,\n",
    "# -33.873992,\n",
    "# -33.866709,\n",
    "# -33.870952,\n",
    "# -33.870530,\n",
    "# -33.867814,\n",
    "# -33.863572,\n",
    "# -33.863264,\n",
    "# -33.857650,\n",
    "# -33.882320,\n",
    "# -33.857132,\n",
    "# -33.929798,\n",
    "# -33.889944,\n",
    "# -33.867841,\n",
    "# -33.851706,\n",
    "# -33.848177,\n",
    "# -33.845931,\n",
    "# -33.842259,\n",
    "# -33.847712,\n",
    "# -33.845917,\n",
    "# -33.842103,\n",
    "# -33.842174,\n",
    "# -33.822557,\n",
    "# -33.815283,\n",
    "# -33.819420,\n",
    "# -33.806476,\n",
    "# -33.809528,\n",
    "# -33.802982,\n",
    "# -33.798890,\n",
    "# -33.800174,\n",
    "# -33.807448,\n",
    "# -33.818812,\n",
    "# -33.796970,\n",
    "# -33.801214,\n",
    "# -33.781750,\n",
    "# -33.776296,\n",
    "# -33.771480,\n",
    "# -33.763881,\n",
    "# -33.757994,\n",
    "# -33.753890,\n",
    "# -33.737696,\n",
    "# -33.712838,\n",
    "# -33.699854,\n",
    "# -33.676179,\n",
    "# -33.656264,\n",
    "# -33.648387,\n",
    "# -33.643315,\n",
    "# -33.635445,\n",
    "# -33.629578,\n",
    "# -33.610213,\n",
    "# -33.597467,\n",
    "# -33.601720,\n",
    "# -33.592676,\n",
    "# -33.616697,\n",
    "# -33.636034,\n",
    "# -33.648470,\n",
    "# -33.658635,\n",
    "# -33.663135,\n",
    "# -33.663420,\n",
    "# -33.661062,\n",
    "# -33.655525,\n",
    "# -33.650202,\n",
    "# -33.652795,\n",
    "# -33.644935,\n",
    "# -33.89318,\n",
    "# -33.63151,\n",
    "# -33.77904,\n",
    "# -33.82199,\n",
    "# -33.51341,\n",
    "# -33.75837,\n",
    "# -33.78945,\n",
    "# -33.867448,\n",
    "# -33.866719,\n",
    "# -33.874093,\n",
    "# -33.879372,\n",
    "# -33.85503,\n",
    "# -33.840582,\n",
    "# -33.596905,\n",
    "# -33.816517,\n",
    "# -34.008981,\n",
    "# -34.013238,\n",
    "# -34.04521,\n",
    "# -34.013287,\n",
    "# -34.017857,\n",
    "# -34.007698,\n",
    "# -33.991254,\n",
    "# -33.998121,\n",
    "# -34.078135,\n",
    "# -34.081702,\n",
    "# -34.08401,\n",
    "# -33.766286,\n",
    "# -33.517781]\n",
    "\n",
    "# exp_lon = [151.228079,\n",
    "# 151.230515,\n",
    "# 151.235563,\n",
    "# 151.238403,\n",
    "# 151.251755,\n",
    "# 151.251985,\n",
    "# 151.262270,\n",
    "# 151.261380,\n",
    "# 151.259680,\n",
    "# 151.257626,\n",
    "# 151.261164,\n",
    "# 151.267136,\n",
    "# 151.266393,\n",
    "# 151.270119,\n",
    "# 151.269946,\n",
    "# 151.268228,\n",
    "# 151.276042,\n",
    "# 151.284334,\n",
    "# 151.282448,\n",
    "# 151.283521,\n",
    "# 151.283440,\n",
    "# 151.284655,\n",
    "# 151.286993,\n",
    "# 151.284930,\n",
    "# 151.280265,\n",
    "# 151.278882,\n",
    "# 151.281823,\n",
    "# 151.280447,\n",
    "# 151.273639,\n",
    "# 151.266851,\n",
    "# 151.268921,\n",
    "# 151.253410,\n",
    "# 151.252892,\n",
    "# 151.244632,\n",
    "# 151.239971,\n",
    "# 151.235620,\n",
    "# 151.230513,\n",
    "# 151.227327,\n",
    "# 151.226874,\n",
    "# 151.217714,\n",
    "# 151.214644,\n",
    "# 151.263990,\n",
    "# 151.268733,\n",
    "# 151.259667,\n",
    "# 151.278405,\n",
    "# 151.238425,\n",
    "# 151.218060,\n",
    "# 151.219562,\n",
    "# 151.224025,\n",
    "# 151.216129,\n",
    "# 151.203984,\n",
    "# 151.229883,\n",
    "# 151.233531,\n",
    "# 151.250998,\n",
    "# 151.251008,\n",
    "# 151.248691,\n",
    "# 151.223979,\n",
    "# 151.229773,\n",
    "# 151.237887,\n",
    "# 151.236042,\n",
    "# 151.282680,\n",
    "# 151.273797,\n",
    "# 151.284354,\n",
    "# 151.293565,\n",
    "# 151.288467,\n",
    "# 151.297651,\n",
    "# 151.290060,\n",
    "# 151.293345,\n",
    "# 151.294203,\n",
    "# 151.301670,\n",
    "# 151.299910,\n",
    "# 151.297206,\n",
    "# 151.306128,\n",
    "# 151.301321,\n",
    "# 151.308997,\n",
    "# 151.315598,\n",
    "# 151.323064,\n",
    "# 151.326921,\n",
    "# 151.330597,\n",
    "# 151.332489,\n",
    "# 151.339247,\n",
    "# 151.331517,\n",
    "# 151.324709,\n",
    "# 151.316941,\n",
    "# 151.320675,\n",
    "# 151.317227,\n",
    "# 151.306587,\n",
    "# 151.302993,\n",
    "# 151.307918,\n",
    "# 151.310278,\n",
    "# 151.305815,\n",
    "# 151.300923,\n",
    "# 151.293713,\n",
    "# 151.286889,\n",
    "# 151.277286,\n",
    "# 151.283766,\n",
    "# 151.26875,\n",
    "# 151.31457,\n",
    "# 151.12056,\n",
    "# 151.19739,\n",
    "# 150.90451,\n",
    "# 151.24892,\n",
    "# 151.28486,\n",
    "# 151.250007,\n",
    "# 151.253684,\n",
    "# 151.245861,\n",
    "# 151.250331,\n",
    "# 151.274354,\n",
    "# 151.248566,\n",
    "# 151.324065,\n",
    "# 151.249134,\n",
    "# 151.191907,\n",
    "# 151.202607,\n",
    "# 151.159248,\n",
    "# 151.113493,\n",
    "# 151.115124,\n",
    "# 151.086539,\n",
    "# 151.128968,\n",
    "# 151.135473,\n",
    "# 151.129342,\n",
    "# 151.14592,\n",
    "# 151.152535,\n",
    "# 150.653403,\n",
    "# 151.176154]\n",
    "\n",
    "# exp_coords = pd.DataFrame({\n",
    "#     'latitude':exp_lat,\n",
    "#     'longitude':exp_lon\n",
    "# })\n",
    "\n",
    "# dist_list = []\n",
    "# import geopy.distance\n",
    "# hot_locs = exp_coords.copy()\n",
    "# all_coords = fe_df[['latitude', 'longitude']].copy()\n",
    "# for index, row in all_coords.iterrows():\n",
    "#     airbnb_loc = (row['latitude'], row['longitude'])\n",
    "#     # this is our constant to compare against so we can grab min in below loop\n",
    "#     starting_diff = geopy.distance.geodesic(airbnb_loc, (hot_locs.loc[0]['latitude'], hot_locs.loc[0]['longitude'])).km\n",
    "#     # will use this to change and appen to min dist list\n",
    "#     diff = starting_diff\n",
    "#     for index, row in hot_locs.iterrows():\n",
    "#         hotloc_coord = (row['latitude'], row['longitude'])\n",
    "#         if geopy.distance.geodesic(airbnb_loc, hotloc_coord).km < diff:\n",
    "#             diff = round(geopy.distance.geodesic(airbnb_loc, hotloc_coord).km,3)\n",
    "#     dist_list.append(diff)\n",
    "    \n",
    "# dist_df = pd.DataFrame(dist_list)\n",
    "# dist_df.columns = ['dist']\n",
    "# dist_df.to_excel('final_distances.xlsx')\n",
    "# #dist_df = pd.read_excel('final_distances.xlsx', index_col = 0)\n",
    "\n",
    "# mtrway_lat = [\n",
    "#     -33.829912,\n",
    "# -33.842568,\n",
    "# -33.857561,\n",
    "# -33.865162,\n",
    "# -33.877167,\n",
    "# -33.81623,\n",
    "# -33.797706,\n",
    "# -33.758232,\n",
    "# -33.754883,\n",
    "# -33.738626,\n",
    "# -33.751075,\n",
    "# -33.950768,\n",
    "# -33.938074,\n",
    "# -33.925247,\n",
    "# -33.941688\n",
    "# ]\n",
    "\n",
    "# mtrway_lon = [\n",
    "#     151.018103,\n",
    "# 151.043708,\n",
    "# 151.072131,\n",
    "# 151.094252,\n",
    "# 151.12962,\n",
    "# 150.963331,\n",
    "# 150.854017,\n",
    "# 151.049254,\n",
    "# 150.953926,\n",
    "# 150.917531,\n",
    "# 150.845879,\n",
    "# 150.876465,\n",
    "# 150.913459,\n",
    "# 150.921265,\n",
    "# 150.942502\n",
    "# ]\n",
    "\n",
    "# highway_coords = pd.DataFrame({\n",
    "#     'latitude':mtrway_lat,\n",
    "#     'longitude':mtrway_lon\n",
    "# })\n",
    "\n",
    "# dist_list = []\n",
    "# import geopy.distance\n",
    "# all_coords = fe_df[['latitude', 'longitude']].copy()\n",
    "\n",
    "# for index, row in all_coords.iterrows():\n",
    "#     airbnb_loc = (row['latitude'], row['longitude'])\n",
    "#     # this is our constant to compare against so we can grab min in below loop\n",
    "#     starting_diff = geopy.distance.geodesic(airbnb_loc, (highway_coords.loc[0]['latitude'], highway_coords.loc[0]['longitude'])).km\n",
    "#     # will use this to change and appen to min dist list\n",
    "#     diff = starting_diff\n",
    "    \n",
    "#     for index, row in highway_coords.iterrows():\n",
    "#         hotloc_coord = (row['latitude'], row['longitude'])\n",
    "#         if geopy.distance.geodesic(airbnb_loc, hotloc_coord).km < diff:\n",
    "#             diff = round(geopy.distance.geodesic(airbnb_loc, hotloc_coord).km,3)\n",
    "    \n",
    "#     dist_list.append(diff)\n",
    "    \n",
    "# hway_dist_df = pd.DataFrame(dist_list)\n",
    "# hway_dist_df.columns = ['hway_dist']\n",
    "# hway_dist_df.to_excel('hway_distances.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# cols_to_drop = ['description', 'neighborhood_overview', 'neighbourhood_cleansed', 'host_about', 'amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20880, 151) (20880,)\n"
     ]
    }
   ],
   "source": [
    "x_fe = fe_df_dums.drop(columns = 'price').copy()\n",
    "y_fe = fe_df_dums['price'].copy()\n",
    "\n",
    "print(x_fe.shape, y_fe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20880, 125)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_df = pd.read_csv('../data/original_data/listing.csv')\n",
    "# clean bathroom column and convert to float\n",
    "default_df['bathrooms'] = default_df['bathrooms_text']\n",
    "default_df.drop(columns = 'bathrooms_text', inplace = True)\n",
    "\n",
    "# drop text or useless cols\n",
    "cols_to_drop = ['id','description','name', 'neighborhood_overview', 'host_name',\n",
    "                'host_location','host_about','host_neighbourhood','host_verifications',\n",
    "                'host_picture_url','id', 'listing_url', 'scrape_id', 'last_scraped', 'picture_url', \n",
    "                'host_id', 'host_url', 'host_thumbnail_url', 'calendar_last_scraped', 'calendar_updated', 'neighbourhood_group_cleansed', \n",
    "                'neighbourhood','property_type','amenities','license']\n",
    "default_df.drop(columns = cols_to_drop, inplace=True)\n",
    "\n",
    "# fix price\n",
    "default_df['price'] = pd.to_numeric(default_df['price'].str.replace('[^.0-9]', ''))\n",
    "\n",
    "# format percentages \n",
    "default_df['host_response_rate'] = default_df['host_response_rate'].str.rstrip('%').astype('float') / 100.0\n",
    "default_df['host_acceptance_rate'] = default_df['host_acceptance_rate'].str.rstrip('%').astype('float') / 100.0\n",
    "\n",
    "# map binary columns appropriately\n",
    "default_df['host_is_superhost'] = default_df['host_is_superhost'].map({'f':0, 't':1})\n",
    "default_df['host_has_profile_pic'] = default_df['host_has_profile_pic'].map({'f':0, 't':1})\n",
    "default_df['host_identity_verified'] = default_df['host_identity_verified'].map({'f':0, 't':1})\n",
    "default_df['has_availability'] = default_df['has_availability'].map({'f':0, 't':1})\n",
    "default_df['instant_bookable'] = default_df['instant_bookable'].map({'f':0, 't':1})\n",
    "\n",
    "# map date columns appropriately \n",
    "default_df['host_since_year'] = pd.to_datetime(default_df['host_since']).dt.year\n",
    "default_df['host_since'] = -(pd.to_datetime(default_df['host_since']) -  pd.to_datetime(\"now\")).dt.days\n",
    "default_df['first_review'] = -(pd.to_datetime(default_df['first_review']) -  pd.to_datetime(\"now\")).dt.days\n",
    "default_df['last_review'] = -(pd.to_datetime(default_df['last_review']) -  pd.to_datetime(\"now\")).dt.days\n",
    "\n",
    "# imputing missing cols with common sense substitution from other column\n",
    "default_df['minimum_minimum_nights'] = default_df['minimum_minimum_nights'].fillna(default_df['minimum_nights'])\n",
    "default_df['maximum_maximum_nights'] = default_df['maximum_maximum_nights'].fillna(default_df['maximum_nights'])\n",
    "default_df['availability_365'] = default_df['availability_365'].fillna(default_df['availability_90']*4) # can also fiddle with just 1 instead of 4 (did analysis)\n",
    "\n",
    "default_df = pd.get_dummies(default_df)\n",
    "default_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix gremlins\n",
    "y_pred[y_pred<20] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S1_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
